{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os, json, requests\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vegardbs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vegardbs/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \",raw)\n",
    "    words = clean.split()\n",
    "    return list(map(lambda x:x.lower(), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principles of Geology, from Gutenberg\n",
    "filepath = \"http://www.gutenberg.org/files/33224/33224-0.txt\"\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "corpus_raw = requests.get(filepath).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Principles of Geology, by Charles Lyell\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give i'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the raw text data from gutenberg, given as a string\n",
    "corpus_raw[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total number of characters in the raw data is\n",
    "clean_raw = re.sub(\"[^a-zA-Z]\",\" \",corpus_raw.lower())\n",
    "words = word_tokenize(clean_raw)\n",
    "unique_words = len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in raw data is 425782, and there are 16977 unique words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of words in raw data is {len(words)}, and there are {unique_words} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use nltk to split the raw data into sentences\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Principles of Geology, by Charles Lyell\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first sentence is\n",
    "raw_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we replace all irregular notation by spaces\n",
    "clean_sentences = []\n",
    "for sentences in raw_sentences:\n",
    "    clean_sentence = re.sub(\"[^a-zA-Z]\",\" \",sentences)\n",
    "    clean_sentences.append(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Project Gutenberg EBook of Principles of Geology  by Charles Lyell    This eBook is for the use of anyone anywhere at no cost and with  almost no restrictions whatsoever '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize sentences\n",
    "sentences = []\n",
    "for sentence in clean_sentences:\n",
    "    sentences.append(sentence.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18225 sentences in the data.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(sentences)} sentences in the training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "\n",
    "#Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "context_size = 7\n",
    "\n",
    "downsampling = 1e-3\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec  = w2v.Word2Vec(\n",
    "                sg = 1,\n",
    "                seed = seed,\n",
    "                workers = num_workers,\n",
    "                size = num_features,\n",
    "                min_count = min_word_count,\n",
    "                window = context_size,\n",
    "                sample = downsampling)\n",
    "model2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"trained\",'sample')):\n",
    "    os.makedirs(os.path.join(\"trained\",'sample',\".w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29989093, 42563300)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2vec.train(sentences, total_examples=model2vec.corpus_count, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vegardbs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gneiss', 0.4801788926124573),\n",
       " ('schists', 0.3700888156890869),\n",
       " ('weigh', 0.36876243352890015),\n",
       " ('porphyry', 0.34800484776496887),\n",
       " ('trap', 0.34250277280807495),\n",
       " ('glen', 0.3391730487346649),\n",
       " ('hartz', 0.33399856090545654),\n",
       " ('mica', 0.3324624300003052),\n",
       " ('rock', 0.33182093501091003),\n",
       " ('schist', 0.3296322822570801)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2vec.most_similar(\"granite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
